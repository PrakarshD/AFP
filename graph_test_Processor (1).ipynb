{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "from graph_tool.all import *\n",
    "import json\n",
    "tqdm.pandas()\n",
    "from IPython.display import Image\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_ID_to_User(ID_dict, ID):\n",
    "    \"\"\"\n",
    "    Function to return the user name from Tweeter user ID\n",
    "    using the ID_dictionary; \n",
    "    Returns NaN if the user is not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ID_dict[ID]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_tweets_processor(df_tweets, period_duration_days = 30):\n",
    "    # Noting the bounds of the time index\n",
    "    start_date = pd.to_datetime('2016-01-01')\n",
    "    curr_date = start_date\n",
    "    end_date = pd.to_datetime('2020-12-31')\n",
    "    \n",
    "    # List to store the Time varying information\n",
    "    weekly_tweets_info = []\n",
    "    \n",
    "    # Traversing through time; time-step defined within the loop\n",
    "    while(curr_date <= end_date):\n",
    "        period_end = curr_date + timedelta(days = period_duration_days)\n",
    "      \n",
    "\n",
    "        df_temp = df_tweets.loc[curr_date:period_end]\n",
    "        \n",
    "        if df_temp.shape[0] > 0 :\n",
    "            std_centrality, links_num, interactions = tweets_sub_processor(df_temp)\n",
    "            weekly_tweets_info.append([period_end, std_centrality, links_num, interactions])\n",
    "        else:\n",
    "            weekly_tweets_info.append([period_end, np.NaN, np.NaN, np.NaN])\n",
    "\n",
    "        curr_date = period_end\n",
    "        \n",
    "        weekly_tweets_df = pd.DataFrame(weekly_tweets_info, columns = ['date', 'stddev_centrality', 'avg_links', 'interactions'])\n",
    "        weekly_tweets_df.set_index('date', inplace = True)\n",
    "        \n",
    "    return weekly_tweets_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_sub_processor(df_tweets_):\n",
    "    \n",
    "    # Selecting a subset of the data to extract and store the Tweets author/originator name\n",
    "    df_IDs_Users_Tweetsid = df_tweets_[['id','user','conversationId']]\n",
    "\n",
    "    # Converting the Tweet ID and Conversation ID to string type\n",
    "    df_IDs_Users_Tweetsid.loc[:,'id'] = df_IDs_Users_Tweetsid.loc[:,'id'].astype(str)\n",
    "    df_IDs_Users_Tweetsid.loc[:,'conversationId'] = df_IDs_Users_Tweetsid.loc[:,'conversationId'].astype(str)\n",
    "\n",
    "    # Extracting the user name and storing it under username \n",
    "    df_IDs_Users_Tweetsid.loc[:,'user_name'] = df_IDs_Users_Tweetsid.loc[:,'user'].apply(lambda user_dict: user_dict['username'])\n",
    "    \n",
    "    # Extracting User_names and tweetIDs seperately as a list\n",
    "    user_list = list(df_IDs_Users_Tweetsid['user_name'].values)\n",
    "    tweets_ID_list = list(df_IDs_Users_Tweetsid['id'].astype(str).values)\n",
    "\n",
    "    # Constructing a dictionary of tweet IDs and user names\n",
    "    dict_tweetsid_username = {}\n",
    "    for i in range(len(user_list)):\n",
    "        dict_tweetsid_username[tweets_ID_list[i]] = user_list[i]\n",
    "\n",
    "    # Extracting and storing the tweets originators/ orginal authors\n",
    "    df_IDs_Users_Tweetsid.loc[:,'tweet_author'] = df_IDs_Users_Tweetsid.loc[:,'conversationId'].progress_apply(lambda conversationId: tweet_ID_to_User(dict_tweetsid_username, conversationId))\n",
    "\n",
    "    # Removing NaN containing rows\n",
    "    df_IDs_Users_Tweetsid = df_IDs_Users_Tweetsid.dropna(how='any', axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Capturing the unique users list\n",
    "    unique_user = list(np.unique(df_IDs_Users_Tweetsid.user_name))\n",
    "    \n",
    "    # Forming a matrix of # of unique_users x # of unique_users\n",
    "    matrix = np.zeros((len(unique_user),len(unique_user)))\n",
    "\n",
    "    # Accounting the tweet interactions from the original authors and replying audience\n",
    "    for i in tqdm(range(df_IDs_Users_Tweetsid.shape[0])):\n",
    "        try:\n",
    "            matrix[unique_user.index(df_IDs_Users_Tweetsid.loc[i,'tweet_author']), unique_user.index(df_IDs_Users_Tweetsid.loc[i,'user_name'])] += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # Number of interaction between 2 different people\n",
    "    interactions = np.sum(matrix - np.diag(np.diag(matrix)))\n",
    "    \n",
    "    # Number of unique users\n",
    "    unique_users = matrix.shape[0]\n",
    "    \n",
    "    # Graph Library object instantiation\n",
    "    g1 = Graph()\n",
    "\n",
    "    # Dict to store the vertices for all the unique users\n",
    "    dict_vertex = {}\n",
    "    for user in unique_user:\n",
    "        dict_vertex[user] = g1.add_vertex()\n",
    "\n",
    "    # Removing duplicate entries of tweets/tweet_replies\n",
    "    dict_edges = {}\n",
    "    df_IDs_Users_Tweets_unique = df_IDs_Users_Tweetsid.drop_duplicates(subset=['user_name','tweet_author'], keep='first').reset_index(drop=True)\n",
    "\n",
    "    for i in range(df_IDs_Users_Tweets_unique.shape[0]):\n",
    "\n",
    "        # Neglecting the initial tweets where the originator or original author makes the tweets; Adding Edges for remaining tweets interactions\n",
    "        if df_IDs_Users_Tweets_unique.loc[i,'user_name'] != df_IDs_Users_Tweets_unique.loc[i,'tweet_author']:\n",
    "            dict_edges[i] = g1.add_edge(dict_vertex[df_IDs_Users_Tweets_unique.loc[i,'user_name']], dict_vertex[df_IDs_Users_Tweets_unique.loc[i,'tweet_author']])\n",
    "    \n",
    "    # PageRank Centrality measure of the graph\n",
    "    pr = pagerank(g1)\n",
    "\n",
    "    # Sorting the most influencial users\n",
    "    df_page_rank = pd.DataFrame(list(pr.a), columns = ['page_rank score'], index = unique_user).sort_values(ascending= False, by='page_rank score')\n",
    "    \n",
    "    # Std Dev of centrality\n",
    "    std_centrality = df_page_rank.std(axis = 0).values[0]\n",
    "    \n",
    "    # Number of links\n",
    "    links_num = np.sum(np.sign((matrix - np.diag(np.diag(matrix))).ravel()))\n",
    "  \n",
    "    \n",
    "    return std_centrality, links_num, interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_pickle('gs://afp_bucket/SP_500/CMG.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123167, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timedelta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-de2d59588414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tweets_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Sample_tests_files/CMG_net_results_monthly.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-24847a7f773d>\u001b[0m in \u001b[0;36mdf_tweets_processor\u001b[0;34m(df_tweets, period_duration_days)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Traversing through time; time-step defined within the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_date\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mperiod_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_date\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperiod_duration_days\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timedelta' is not defined"
     ]
    }
   ],
   "source": [
    "df_tweets.set_index('date', inplace= True)\n",
    "df_tweets.sort_index(inplace= True)\n",
    "df_info = df_tweets_processor(df_tweets)\n",
    "df_info.to_pickle('./Sample_tests_files/CMG_net_results_monthly.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.mnightly-2021-01-05-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:mnightly-2021-01-05-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python [conda env:graph]",
   "language": "python",
   "name": "conda-env-graph-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
